{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7tlHdwY5EHBY",
        "outputId": "44bb9036-4c42-437f-f73a-5054c5825ed5"
      },
      "outputs": [],
      "source": [
        "# !pip install -U transformers datasets accelerate torch numpy fastai scikit-learn tqdm\n",
        "# # Optional (for 4-bit quantization on GPU):\n",
        "# !pip install -U bitsandbytes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference on Train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def _tri(L_in, L_out):\n",
        "    # attention sweep over prefill + generation with KV cache\n",
        "    return (L_in * (L_in + 1) / 2.0) + (L_out * L_in) + ((L_out - 1) * L_out / 2.0)\n",
        "\n",
        "def _arch_constants_from_config(cfg):\n",
        "    \"\"\"\n",
        "    Infer c_mlp (d^2-heavy) and c_attn (d·L) from HF config.\n",
        "    - c_mlp = (projections) + (MLP)\n",
        "      projections: Q,O full-size + K,V reduced by r_kv = n_kv_heads / n_heads\n",
        "        => proj_factor ≈ 2 + 2 * r_kv   (≈4 when MHA; smaller when GQA/MQA)\n",
        "      MLP: use expansion r = intermediate_size / hidden_size\n",
        "        GeLU-like => mlp_factor ≈ 2 * r\n",
        "        SwiGLU/SiLU-like => mlp_factor ≈ 3 * r\n",
        "    - c_attn: ~2 (QK^T + AV). Keep a small mid-range default.\n",
        "    \"\"\"\n",
        "    d = getattr(cfg, \"hidden_size\", None)\n",
        "    if d is None:\n",
        "      d = getattr(cfg.text_config, \"hidden_size\")\n",
        "    n = getattr(cfg, \"num_hidden_layers\", None)\n",
        "    if n is None:\n",
        "      n = getattr(cfg.text_config, \"num_hidden_layers\")\n",
        "    inter = getattr(cfg, \"intermediate_size\", 4 * d)\n",
        "    r = inter / d\n",
        "\n",
        "    act = (getattr(cfg, \"hidden_activation\", \"\") or \"\").lower()\n",
        "    if \"swiglu\" in act or \"silu\" in act or \"swish\" in act:\n",
        "        mlp_factor = 3.0 * r\n",
        "    else:\n",
        "        mlp_factor = 2.0 * r\n",
        "\n",
        "    h = getattr(cfg, \"num_attention_heads\", None)\n",
        "    h_kv = getattr(cfg, \"num_key_value_heads\", h)\n",
        "    r_kv = (h_kv / h) if (h and h_kv) else 1.0\n",
        "    proj_factor = 2.0 + 2.0 * r_kv  # Q,O full (2) + K,V scaled by r_kv\n",
        "\n",
        "    c_mlp = proj_factor + mlp_factor   # total d^2-heavy constant\n",
        "    c_attn = 2.5                       # mild mid-point for attention kernels\n",
        "    return n, d, c_mlp, c_attn\n",
        "\n",
        "def query_cost(config, L_in: int, L_out: int, mode: str = \"tflops\",\n",
        "               count_mac_as_2flop: bool = True):\n",
        "    \"\"\"\n",
        "    Returns a single scalar per-query cost.\n",
        "    mode=\"units\"  -> unitless, param-free, architecture-agnostic\n",
        "    mode=\"tflops\" -> weighted & scaled (uses config to estimate constants)\n",
        "    \"\"\"\n",
        "    n = getattr(config, \"num_hidden_layers\", None)\n",
        "    if n is None:\n",
        "      n = getattr(config.text_config, 'num_hidden_layers')\n",
        "    d = getattr(config, \"hidden_size\", None)\n",
        "    if d is None:\n",
        "      d = getattr(config.text_config, 'hidden_size')\n",
        "    tri = _tri(L_in, L_out)\n",
        "\n",
        "    if mode == \"units\":\n",
        "        return n * ((L_in + L_out) * (d ** 2) + d * tri)\n",
        "\n",
        "    # tflops: architecture-aware constants + MAC->FLOPs + scale\n",
        "    n, d, c_mlp, c_attn = _arch_constants_from_config(config)\n",
        "    a = n * c_mlp * (d ** 2)     # d^2-heavy piece\n",
        "    b = n * c_attn * d           # d·L attention piece\n",
        "    flops = (L_in + L_out) * a + b * tri\n",
        "    if count_mac_as_2flop:\n",
        "        flops *= 2.0\n",
        "    return flops / 1e12  # TFLOPs-ish\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j1UYjRe7F1aF",
        "outputId": "b9151b94-168e-47ea-fd7a-ac3bfa65b1ac"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "import torch\n",
        "import torch.profiler as prof\n",
        "import torch.nn as nn\n",
        "\n",
        "from datasets import load_dataset\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
        "from transformers import MistralCommonBackend, FineGrainedFP8Config, Mistral3ForConditionalGeneration\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, f1_score, mean_squared_error, r2_score, mean_absolute_error\n",
        "from collections import Counter\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 1) Config\n",
        "# -----------------------------\n",
        "MODEL_ID = os.environ.get(\"MODEL_ID\", \"google/gemma-3-1b-it\")  # swap if needed\n",
        "# MODEL_ID = 'mistralai/Ministral-3-3B-Instruct-2512'\n",
        "DATASET_ID = \"rajpurkar/squad\"\n",
        "\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DTYPE = torch.float16 if (DEVICE == \"cuda\") else torch.float32\n",
        "\n",
        "BATCH_SIZE = 4 if DEVICE == \"cuda\" else 2\n",
        "MAX_SAMPLES = 8000\n",
        "MAX_NEW_TOKENS = 9\n",
        "SEED = 42\n",
        "\n",
        "# Embedding supervision threshold (Option 2)\n",
        "SIM_THRESHOLD = 0.7\n",
        "\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 2) Quantile features (paper-style)\n",
        "#    [0, 0.01..0.1 step 0.01, 0.2..1.0 step 0.1] => 20 quantiles\n",
        "# -----------------------------\n",
        "ALPHAS = [0.0] + [i / 100 for i in range(1, 11)] + [i / 10 for i in range(2, 11)]\n",
        "assert len(ALPHAS) == 20\n",
        "\n",
        "def build_posthoc_quantile_features(token_logprobs: np.ndarray) -> np.ndarray:\n",
        "    \"\"\"\n",
        "    token_logprobs: shape (T,), log p(y_t | context) for generated answer tokens\n",
        "    returns: shape (22,) = [sum, avg] + 20 quantiles\n",
        "    \"\"\"\n",
        "    if token_logprobs.size == 0:\n",
        "        token_logprobs = np.array([-1e9], dtype=np.float32)\n",
        "\n",
        "    s_sum = float(token_logprobs.sum())\n",
        "    s_avg = float(token_logprobs.mean())\n",
        "    qs = np.quantile(token_logprobs, ALPHAS).astype(np.float32)\n",
        "\n",
        "    feats = np.concatenate([[s_sum, s_avg], qs], axis=0).astype(np.float32)\n",
        "    # feats = qs.astype(np.float32)\n",
        "    return feats\n",
        "\n",
        "def to_tensor(x_np, device):\n",
        "    return torch.tensor(x_np, dtype=torch.float32, device=device)\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 3) Prompting for QA\n",
        "# -----------------------------\n",
        "def make_prompt(context: str, question: str) -> str:\n",
        "    return (\n",
        "        \"You are a helpful assistant. Answer the question using the context.\\n\\n\"\n",
        "        f\"Context:\\n{context}\\n\\n\"\n",
        "        f\"Question: {question}\\n\"\n",
        "        \"Answer (be concise):\"\n",
        "    )\n",
        "\n",
        "def make_prompt2(context: str, question: str) -> str:\n",
        "    return f\"\"\"\n",
        "You are a question answering assistant.\n",
        "Your task is to answer the question based on the provided context. \\n\\n\n",
        "Rules:\n",
        "\n",
        "1. Use only information explicitly stated in the context.\n",
        "\n",
        "2. Do not use external knowledge or assumptions.\n",
        "\n",
        "3. Keep the answer short and precise, ideally a phrase or sentence directly supported by the text.\n",
        "\n",
        "4. When possible, copy the answer verbatim from the context rather than paraphrasing.\\n\n",
        "\n",
        "Context:\n",
        "{context}\n",
        "\n",
        "Question:\n",
        "{question}\n",
        "\n",
        "Answer (one short phrase): \n",
        "\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 4) Load tokenizer/model\n",
        "# -----------------------------\n",
        "# tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, use_fast=True)\n",
        "tokenizer = MistralCommonBackend.from_pretrained(MODEL_ID)  # for ministral\n",
        "# tokenizer.padding_side = \"left\"\n",
        "# if tokenizer.pad_token_id is None:\n",
        "#     tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "# use_4bit = (DEVICE == \"cuda\") and (os.environ.get(\"USE_4BIT\", \"0\") == \"1\")\n",
        "use_4bit = False\n",
        "if use_4bit:\n",
        "    from transformers import BitsAndBytesConfig\n",
        "    quant_cfg = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16)\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        MODEL_ID, quantization_config=quant_cfg, device_map=\"auto\"\n",
        "    )\n",
        "else:\n",
        "    model = AutoModelForCausalLM.from_pretrained(MODEL_ID, dtype=DTYPE).to(DEVICE)\n",
        "    # model = Mistral3ForConditionalGeneration.from_pretrained(\n",
        "    #     MODEL_ID,\n",
        "    #     # quantization_config=quantization_config,\n",
        "    #     # torch_dtype=torch.bfloat16,\n",
        "    #     device_map=\"auto\",\n",
        "    #     quantization_config=FineGrainedFP8Config(dequantize=True)  # for ministral\n",
        "    # )\n",
        "\n",
        "model.eval()\n",
        "if tokenizer.pad_token is None:\n",
        "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "def normalize_answer(s: str) -> str:\n",
        "    import string\n",
        "    \"\"\"\n",
        "    SQuAD-style normalization:\n",
        "    - lowercase\n",
        "    - remove punctuation\n",
        "    - remove articles (a, an, the)\n",
        "    - collapse whitespace\n",
        "    \"\"\"\n",
        "    if s is None:\n",
        "        return \"\"\n",
        "\n",
        "    def lower(text: str) -> str:\n",
        "        return text.lower()\n",
        "\n",
        "    def remove_punc(text: str) -> str:\n",
        "        exclude = set(string.punctuation)\n",
        "        return \"\".join(ch for ch in text if ch not in exclude)\n",
        "\n",
        "    def remove_articles(text: str) -> str:\n",
        "        return re.sub(r\"\\b(a|an|the)\\b\", \" \", text)\n",
        "\n",
        "    def white_space_fix(text: str) -> str:\n",
        "        return \" \".join(text.split())\n",
        "\n",
        "    return white_space_fix(remove_articles(remove_punc(lower(s))))\n",
        "\n",
        "# -----------------------------\n",
        "# 5) Embedding model for Option 2 supervision\n",
        "# -----------------------------\n",
        "# This model is small, fast, and widely used for semantic similarity.\n",
        "embedder = SentenceTransformer(\"sentence-transformers/all-mpnet-base-v2\", device=DEVICE)\n",
        "# embedder = None\n",
        "\n",
        "def cosine_sim(a: np.ndarray, b: np.ndarray) -> float:\n",
        "    # a, b are L2-normalized vectors\n",
        "    return float(np.dot(a, b))\n",
        "\n",
        "def max_gold_similarity(pred: str, gold_list: list[str]) -> float:\n",
        "    \"\"\"\n",
        "    Returns max cosine similarity between pred and any gold in gold_list.\n",
        "    \"\"\"\n",
        "    # Encode all at once for speed; normalize_embeddings=True returns L2-normalized vectors.\n",
        "    # texts = [pred] + (gold_list if len(gold_list) > 0 else [\"\"])\n",
        "    # embs = embedder.encode(texts, normalize_embeddings=True)\n",
        "    # pred_emb = embs[0]\n",
        "    # gold_embs = embs[1:]\n",
        "    # print(pred_emb.shape, gold_embs.shape)\n",
        "\n",
        "    pred_emb = embedder.encode([pred], normalize_embeddings=True).flatten()\n",
        "    gold_embs = embedder.encode((gold_list if len(gold_list) > 0 else [\"\"]), normalize_embeddings=True)\n",
        "    if gold_embs.shape[0] == 0:\n",
        "        return 0.0\n",
        "    # print(pred_emb.shape, gold_embs.shape)\n",
        "    sims = gold_embs @ pred_emb  # because normalized => cosine\n",
        "    return float(np.max(sims))\n",
        "\n",
        "def bert_score(pred: str, gold_lists: list[str]) -> float:\n",
        "    from evaluate import load\n",
        "    bertscore = load(\"bertscore\")\n",
        "    f1_scores = []\n",
        "    for gold in gold_lists:\n",
        "        results = bertscore.compute(predictions=[pred], references=[gold], lang=\"en\", \n",
        "                                    model_type=\"microsoft/deberta-v3-xsmall\", num_layers=10, \n",
        "                                    device='cpu')\n",
        "        f1_scores.append(np.array(results['f1']).mean())\n",
        "    \n",
        "    return float(max(f1_scores))\n",
        "\n",
        "# def normalize_answer(s: str) -> str:\n",
        "#     \"\"\"Lower text and remove punctuation, articles and extra whitespace.\"\"\"\n",
        "#     s = s.lower()\n",
        "#     s = re.sub(r\"\\b(a|an|the)\\b\", \" \", s)\n",
        "#     s = re.sub(r\"[^a-z0-9\\s]\", \" \", s)\n",
        "#     s = \" \".join(s.split())\n",
        "#     return s\n",
        "\n",
        "def token_f1(prediction: str, ground_truth: str) -> float:\n",
        "    pred_tokens = normalize_answer(prediction).split()\n",
        "    gold_tokens = normalize_answer(ground_truth).split()\n",
        "\n",
        "    if len(pred_tokens) == 0 and len(gold_tokens) == 0:\n",
        "        return 1.0\n",
        "    if len(pred_tokens) == 0 or len(gold_tokens) == 0:\n",
        "        return 0.0\n",
        "\n",
        "    common = Counter(pred_tokens) & Counter(gold_tokens)\n",
        "    num_same = sum(common.values())\n",
        "    if num_same == 0:\n",
        "        return 0.0\n",
        "\n",
        "    precision = num_same / len(pred_tokens)\n",
        "    recall = num_same / len(gold_tokens)\n",
        "    return 2 * precision * recall / (precision + recall)\n",
        "\n",
        "def squad_max_f1(prediction: str, gold_list: list[str]) -> float:\n",
        "    \"\"\"SQuAD uses max over all references.\"\"\"\n",
        "    if not gold_list:\n",
        "        return 0.0\n",
        "    return max(token_f1(prediction, gold) for gold in gold_list)\n",
        "\n",
        "\n",
        "\n",
        "from transformers import StoppingCriteria\n",
        "\n",
        "class StopOnAnySubsequence(StoppingCriteria):\n",
        "    def __init__(self, stop_sequences):\n",
        "        \"\"\"\n",
        "        stop_sequences: List[List[int]]\n",
        "            Each element is a token-id sequence to stop on\n",
        "        \"\"\"\n",
        "        self.stop_sequences = stop_sequences\n",
        "        self.max_len = max(len(seq) for seq in stop_sequences)\n",
        "\n",
        "    def __call__(self, input_ids, scores, **kwargs):\n",
        "        # input_ids shape: (batch_size, seq_len)\n",
        "        generated = input_ids[0].tolist()\n",
        "\n",
        "        # Only check the last max_len tokens\n",
        "        tail = generated[-self.max_len:]\n",
        "\n",
        "        for stop_seq in self.stop_sequences:\n",
        "            L = len(stop_seq)\n",
        "            if tail[-L:] == stop_seq:\n",
        "                return True\n",
        "\n",
        "        return False\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 6) Inference: generate + token logprobs\n",
        "# -----------------------------\n",
        "@torch.inference_mode()\n",
        "def generate_with_token_logprobs(prompts, return_results=False, ):\n",
        "    # if return_results:\n",
        "    #     assert mlp is not None\n",
        "    #     mlp = mlp.to(DEVICE)\n",
        "    enc = tokenizer(prompts, return_tensors=\"pt\", padding=True, )\n",
        "    input_ids = enc[\"input_ids\"].to(model.device)\n",
        "    attention_mask = enc[\"attention_mask\"].to(model.device)\n",
        "\n",
        "    input_len = input_ids.shape[1]\n",
        "    L_in_list = attention_mask.sum(dim=1).detach().cpu().tolist()\n",
        "\n",
        "    if return_results:\n",
        "        with prof.profile(\n",
        "            activities=[prof.ProfilerActivity.CPU, prof.ProfilerActivity.CUDA],\n",
        "            with_flops=True,\n",
        "            record_shapes=False\n",
        "        ) as p:\n",
        "            out = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False,                 # greedy\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                # stopping_criteria=[StopOnAnySubsequence(\n",
        "                #     stop_sequences=[\n",
        "                #         tokenizer.encode(\"\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\" \\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"  \\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\\n\", add_special_tokens=False),\n",
        "                #     ]\n",
        "                #     )],\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "        # Aggregate estimated FLOPs across all ops\n",
        "        total_flops = sum(\n",
        "            e.flops for e in p.events() if hasattr(e, \"flops\") and e.flops is not None\n",
        "        )\n",
        "        total_tflops = total_flops / 1e12  # convert to TFLOPs\n",
        "\n",
        "        pad_length = enc[\"input_ids\"].shape[-1]\n",
        "        L_out_list = []\n",
        "        for i in range(len(L_in_list)):\n",
        "          seq_len = out.sequences[i].shape[-1]\n",
        "          L_out = max(seq_len - pad_length, 0)\n",
        "          assert L_out <= MAX_NEW_TOKENS\n",
        "          L_out_list.append(L_out)\n",
        "\n",
        "        # Compute total effective token count across batch\n",
        "        total_tokens = sum(L_in + L_out for L_in, L_out in zip(L_in_list, L_out_list))\n",
        "\n",
        "        # Distribute total FLOPs proportionally to (L_in + L_out)\n",
        "        cost_tflops_list = [\n",
        "            total_tflops * (L_in + L_out) / total_tokens\n",
        "            for L_in, L_out in zip(L_in_list, L_out_list)\n",
        "        ]\n",
        "\n",
        "        # cost_tflops_list = [\n",
        "        #     query_cost(model.config, L_in_list[i], L_out_list[i], mode=\"tflops\")\n",
        "        #     for i in range(len(L_in_list))\n",
        "        # ]\n",
        "\n",
        "    else:\n",
        "        out = model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=MAX_NEW_TOKENS,\n",
        "                do_sample=False,                 # greedy\n",
        "                return_dict_in_generate=True,\n",
        "                output_scores=True,\n",
        "                # stopping_criteria=[StopOnAnySubsequence(\n",
        "                #     stop_sequences=[\n",
        "                #         tokenizer.encode(\"\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\" \\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"  \\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\", add_special_tokens=False),\n",
        "                #         tokenizer.encode(\"\\n\\n\\n\", add_special_tokens=False),\n",
        "                #     ]\n",
        "                #     )],\n",
        "                pad_token_id=tokenizer.pad_token_id,\n",
        "                eos_token_id=tokenizer.eos_token_id,\n",
        "            )\n",
        "\n",
        "    sequences = out.sequences\n",
        "\n",
        "    assert len(sequences) == len(prompts)\n",
        "\n",
        "    # print(f'Batch input length: {input_len}, Batch whole sequence length: {sequences.shape}')\n",
        "    scores = out.scores  # list length = gen_len, each (B, vocab)\n",
        "    gen_len = len(scores)\n",
        "\n",
        "    # print('Generated length:', gen_len)  # this should be up until MAX_NEW_TOKENS\n",
        "    # for i in range(sequences.shape[0]):\n",
        "    #     print(f'Prompt: {tokenizer.decode(sequences[i, :input_len])}')\n",
        "    gen_token_ids = sequences[:, input_len: input_len + gen_len]  # (B, gen_len) --> takes only generated tokens\n",
        "\n",
        "    token_logprobs = []  # batch size\n",
        "    for t in range(gen_len):\n",
        "        step_logits = scores[t]  # (B, vocab)\n",
        "        step_logprobs = torch.log_softmax(step_logits, dim=-1)\n",
        "        tok_t = gen_token_ids[:, t]  # (B,)\n",
        "        lp_t = step_logprobs.gather(1, tok_t.unsqueeze(1)).squeeze(1)  # (B,)\n",
        "        token_logprobs.append(lp_t)\n",
        "\n",
        "    token_logprobs = torch.stack(token_logprobs, dim=1)  # (B, gen_len)\n",
        "\n",
        "    # generated = out[0]\n",
        "    # print(\"EOS present:\", tokenizer.eos_token_id in generated)\n",
        "    # print(\"New line present:\", tokenizer.encode(\"\\n\", add_special_tokens=False)[0] in generated)\n",
        "    # print(\"Length:\", generated.shape[-1])\n",
        "\n",
        "    token_logprobs_list = []\n",
        "    decoded_texts = []\n",
        "    results = []\n",
        "    del input_ids, attention_mask\n",
        "    for i in range(sequences.shape[0]):\n",
        "        toks = gen_token_ids[i].tolist()\n",
        "        lps = token_logprobs[i].detach().cpu().numpy()\n",
        "\n",
        "        cut = gen_len\n",
        "        if tokenizer.eos_token_id in toks:\n",
        "            eos_pos = toks.index(tokenizer.eos_token_id)\n",
        "            cut = eos_pos  # exclude EOS token itself\n",
        "\n",
        "        lps = lps[:cut]\n",
        "        decoded = tokenizer.decode(gen_token_ids[i, :cut], skip_special_tokens=True)\n",
        "        decoded_texts.append(decoded.strip())\n",
        "        token_logprobs_list.append(lps.astype(np.float32))\n",
        "\n",
        "        if return_results:\n",
        "\n",
        "            results.append({\n",
        "                \"text\": prompts[i],\n",
        "                \"text_length\": L_in_list[i],\n",
        "                \"output_length\": L_out_list[i],\n",
        "                \"pred\": decoded,\n",
        "                # \"prob_negative\": probs[i, neg_id].item(),\n",
        "                # \"prob_positive\": probs[i, pos_id].item(),\n",
        "                \"logprobs\": lps,\n",
        "                \"cost_tflops\": cost_tflops_list[i],\n",
        "            })\n",
        "\n",
        "    return decoded_texts, token_logprobs_list, results if results else None\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 7) Build dataset of (features, label) where label is semantic correctness\n",
        "# -----------------------------\n",
        "raw = load_dataset(DATASET_ID)\n",
        "print(raw)\n",
        "raw = raw['train']\n",
        "print(raw)\n",
        "raw = raw.select(range(min(MAX_SAMPLES, len(raw))))\n",
        "print(f'Dataset length: {len(raw)}')\n",
        "\n",
        "features = []\n",
        "labels_clf = []\n",
        "labels_reg = []\n",
        "sims_all = []\n",
        "f1s_all = []\n",
        "\n",
        "results = []\n",
        "\n",
        "for start in tqdm(range(0, len(raw), BATCH_SIZE), desc=\"Generating\"):\n",
        "    batch = raw[start:start + BATCH_SIZE]\n",
        "    prompts = [make_prompt2(c, q) for c, q in zip(batch[\"context\"], batch[\"question\"])]\n",
        "\n",
        "    # preds, token_lps_list, r = generate_with_token_logprobs(prompts)  # batch\n",
        "    preds, token_lps_list, r = generate_with_token_logprobs(prompts, return_results=True)\n",
        "\n",
        "    # SQuAD golds are lists of acceptable spans\n",
        "    gold_lists = [ans[\"text\"] if \"text\" in ans else [\"\"] for ans in batch[\"answers\"]]\n",
        "    # print(gold_lists)\n",
        "    # print(gold_lists[0])\n",
        "\n",
        "    for i, (pred, golds, lps) in enumerate(zip(preds, gold_lists, token_lps_list)):  # traverse each sample in batch\n",
        "        if r is not None:\n",
        "            r[i]['gold_answer'] = golds[0]\n",
        "\n",
        "        # print('Predicted text:', pred)\n",
        "        # print('Gold text:', golds)\n",
        "        feat = build_posthoc_quantile_features(lps)\n",
        "        sim = max_gold_similarity(pred, golds)\n",
        "        y_sim_binary = int(sim >= SIM_THRESHOLD)\n",
        "        # y_sim = float(np.clip(sim, 0.0, 1.0))  # regression!\n",
        "        y_sim = float(np.clip((sim + 1.0) / 2.0, 0.0, 1.0))\n",
        "\n",
        "        f1 = squad_max_f1(pred, golds)\n",
        "        # f1 = bert_score(pred, golds)\n",
        "        y_f1 = int(f1 >= SIM_THRESHOLD)\n",
        "        # y_f1 = f1  # regression!\n",
        "\n",
        "        features.append(feat)\n",
        "        # labels.append(y_f1)\n",
        "        labels_clf.append(y_sim_binary)\n",
        "        labels_reg.append(y_sim)\n",
        "        sims_all.append(sim)\n",
        "        f1s_all.append(f1)\n",
        "    \n",
        "    if r is not None:\n",
        "        results.extend(r) if BATCH_SIZE > 1 else results.append(r)\n",
        "        \n",
        "\n",
        "X = np.stack(features).astype(np.float32)  # (N, 22)\n",
        "y_clf = np.array(labels_clf).astype(np.float32)\n",
        "y_reg = np.array(labels_reg).astype(np.float32)\n",
        "sims_all = np.array(sims_all, dtype=np.float32)\n",
        "f1s_all = np.array(f1s_all, dtype=np.float32)\n",
        "from scipy.stats import spearmanr\n",
        "\n",
        "rho, p = spearmanr(sims_all, f1s_all)\n",
        "print('Spearman correlation of similarities and F1s:', rho)\n",
        "\n",
        "print(\"Dataset size:\", X.shape)\n",
        "print(\"Similarity stats: min/mean/median/max =\",\n",
        "      float(sims_all.min()), float(sims_all.mean()), float(np.median(sims_all)), float(sims_all.max()))\n",
        "print(\"F1 stats: min/mean/median/max =\",\n",
        "      float(f1s_all.min()), float(f1s_all.mean()), float(np.median(f1s_all)), float(f1s_all.max()))\n",
        "print(f\"Positive rate (sim >= {SIM_THRESHOLD}):\", float((sims_all >= SIM_THRESHOLD).mean()))\n",
        "print(f\"Positive rate (F1 >= {SIM_THRESHOLD}):\", float((f1s_all >= SIM_THRESHOLD).mean()))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "avg_lp = X[:,1]          # if built [sum, avg, q...]\n",
        "min_lp = X[:,2]          # q(alpha=0) if that’s first quantile\n",
        "print(\"corr(avg_lp, sim)\", np.corrcoef(avg_lp, sims_all)[0,1])\n",
        "print(\"corr(min_lp, sim)\", np.corrcoef(min_lp, sims_all)[0,1])\n",
        "print(\"sim min/mean/median/max\", sims_all.min(), sims_all.mean(), np.median(sims_all), sims_all.max())\n",
        "print(\"sim percentiles\", np.quantile(sims_all, [0.1,0.25,0.5,0.75,0.9]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OMQK3kZVbSaf",
        "outputId": "847b60de-3655-4a55-ca01-2467c81030a9"
      },
      "outputs": [],
      "source": [
        "import gc\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Quantile MLP (Binary Classification target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna, copy\n",
        "\n",
        "# classification cell --> y = y_clf\n",
        "y = copy.deepcopy(y_clf)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Split + standardize\n",
        "# -----------------------------\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y.flatten(), test_size=0.25, random_state=SEED, \n",
        "#     stratify=(y if y.sum() > 0 else None)\n",
        "# )\n",
        "\n",
        "# split for tuning...\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y,\n",
        "#     test_size=0.2,\n",
        "#     # stratify=(y if y.sum() > 0 else None),\n",
        "#     shuffle=False,\n",
        "#     random_state=SEED\n",
        "# )\n",
        "split_idx = int(0.8 * len(X))\n",
        "\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "mu = X_train.mean(axis=0, keepdims=True)\n",
        "sigma = X_train.std(axis=0, keepdims=True) + 1e-6\n",
        "X_train_std = (X_train - mu) / sigma\n",
        "X_val_std = (X_val - mu) / sigma\n",
        "# X_test_std  = (X_test  - mu) / sigma\n",
        "\n",
        "print(f'Train set: {X_train_std.shape, y_train.shape}')\n",
        "print(f'Val set: {X_val_std.shape, y_val.shape}')\n",
        "# print(f'Test set: {X_test_std.shape, y_test.shape}')\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 9) MLP router\n",
        "# -----------------------------\n",
        "class QuantileMLP(nn.Module):\n",
        "    def __init__(self, in_dim=22, h1=64, h2=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h1, h2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h2, 1),  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "    \n",
        "\n",
        "class QuantileMLPTunable(nn.Module):\n",
        "    def __init__(self, in_dim, layers, dropout, bn):\n",
        "        super().__init__()\n",
        "\n",
        "        net = []\n",
        "        prev_dim = in_dim\n",
        "        for h in layers:\n",
        "            net.append(nn.Linear(prev_dim, h))\n",
        "            if bn:\n",
        "                net.append(nn.BatchNorm1d(h))\n",
        "            net.append(nn.ReLU())\n",
        "            net.append(nn.Dropout(dropout))\n",
        "            prev_dim = h\n",
        "\n",
        "        net.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "def pairwise_ranking_loss(scores, targets, eps=1e-4):\n",
        "    \"\"\"\n",
        "    scores: (B,) router outputs (use raw logits, NOT sigmoid)\n",
        "    targets: (B,) continuous quality labels (similarities)\n",
        "    \"\"\"\n",
        "    # Pairwise differences\n",
        "    ds = scores.unsqueeze(1) - scores.unsqueeze(0)   # (B,B)\n",
        "    dq = targets.unsqueeze(1) - targets.unsqueeze(0) # (B,B)\n",
        "\n",
        "    # Keep only informative pairs (avoid ties)\n",
        "    sign = torch.sign(dq)\n",
        "    mask = (dq.abs() > eps)\n",
        "\n",
        "    # Logistic ranking loss: softplus(-sign * ds)\n",
        "    loss_mat = torch.nn.functional.softplus(-sign * ds)\n",
        "    return loss_mat[mask].mean()\n",
        "\n",
        "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + torch.nn.functional.softplus(-2. * x) - np.log(2.0)\n",
        "    return torch.mean(_log_cosh(y_pred - y_true))\n",
        "\n",
        "class LogCoshLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(\n",
        "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        return log_cosh_loss(y_pred, y_true)\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # ----- Architecture search -----\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "    layers = [\n",
        "        trial.suggest_int(f\"h{i}\", 16, 512, log=False, step=16)\n",
        "        for i in range(n_layers)\n",
        "    ]\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "    bn = trial.suggest_categorical(\"batch_norm\", [False, True])\n",
        "\n",
        "    model = QuantileMLPTunable(\n",
        "        in_dim=X_train.shape[1],\n",
        "        layers=layers,\n",
        "        dropout=dropout,\n",
        "        bn=bn\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # ----- Optimizer -----\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
        "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    threshold = trial.suggest_float(\"threshold\", 0, 1, step=0.1)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    clf_fn = nn.BCEWithLogitsLoss()  # binary classification\n",
        "    # reg_fn = nn.HuberLoss()  # regression\n",
        "\n",
        "    EPOCHS = 250\n",
        "    best_val_score = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = model(to_tensor(X_train_std, DEVICE))\n",
        "        # logits = torch.sigmoid(logits)  # if regression!\n",
        "\n",
        "        loss = clf_fn(logits, to_tensor(y_train, DEVICE))\n",
        "        # loss = reg_fn(torch.sigmoid(logits), to_tensor(y_train, DEVICE))\n",
        "        # loss = 0.8 * pairwise_ranking_loss(logits, \n",
        "        #                                    to_tensor(y_train, DEVICE), eps=0.01) + 0.2 * reg_fn(torch.sigmoid(logits), \n",
        "        #                                                                                         to_tensor(y_train, DEVICE))\n",
        "        # loss = pairwise_ranking_loss(logits, to_tensor(y_train, DEVICE))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # ---- validate ----\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(to_tensor(X_val_std, DEVICE))\n",
        "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
        "\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                val_score = f1_score(y_val, (val_probs >= threshold).astype(np.float32))\n",
        "            else:\n",
        "                # val_f1 = float(\"nan\")\n",
        "                # val_score = spearmanr(y_val, val_probs).statistic  # regression\n",
        "                # val_score = mean_squared_error(y_val, val_probs)\n",
        "                val_score = mean_absolute_error(y_val, val_probs)\n",
        "                # val_score = r2_score(y_val, val_probs)\n",
        "\n",
        "        best_val_score = max(best_val_score, val_score)\n",
        "\n",
        "        # ---- pruning ----\n",
        "        trial.report(val_score, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return best_val_score\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"maximize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best val. score:\", study.best_value)\n",
        "print(\"Best params:\", study.best_params)\n",
        "\n",
        "\n",
        "# define best parameters and train with full data\n",
        "best = study.best_params\n",
        "\n",
        "best_layers = [best[f\"h{i}\"] for i in range(best[\"n_layers\"])]\n",
        "\n",
        "best_mlp = QuantileMLPTunable(\n",
        "    in_dim=X_train_std.shape[1],\n",
        "    layers=best_layers,\n",
        "    dropout=best[\"dropout\"],\n",
        "    bn=best[\"batch_norm\"]\n",
        ").to(DEVICE)\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    best_mlp.parameters(),\n",
        "    lr=best[\"lr\"],\n",
        "    weight_decay=best[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "clf_fn = nn.BCEWithLogitsLoss()  # binary classification\n",
        "# reg_fn = nn.HuberLoss()  # regression\n",
        "\n",
        "X_full = to_tensor(np.vstack((X_train_std, X_val_std)), DEVICE)\n",
        "y_full = to_tensor(np.concatenate((y_train, y_val)), DEVICE)\n",
        "print(f'\\nTrain+Val set: {X_full.shape, y_full.shape}')\n",
        "\n",
        "EPOCHS = 200\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    best_mlp.train()\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    logits = best_mlp(X_full)\n",
        "    # logits = torch.sigmoid(logits)  # regression\n",
        "    \n",
        "    loss = clf_fn(logits, y_full)\n",
        "    # loss = reg_fn(torch.sigmoid(logits), y_full)\n",
        "    # loss = 0.8 * pairwise_ranking_loss(logits, \n",
        "    #                                    to_tensor(y_full, DEVICE), eps=0.01) + 0.2 * reg_fn(torch.sigmoid(logits), \n",
        "    #                                                                                         to_tensor(y_full, DEVICE))\n",
        "    # loss = pairwise_ranking_loss(logits, to_tensor(y_full, DEVICE))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    f1_tr = f1_score(y_full.cpu().numpy(), (torch.sigmoid(logits).detach().cpu().numpy() >= best['threshold']))\n",
        "    # corr_tr = spearmanr(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy()).statistic\n",
        "    # mse_tr = mean_squared_error(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "    # mae_tr = mean_absolute_error(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "    # r2_tr = r2_score(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "\n",
        "    # best_mlp.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     test_logits = best_mlp(to_tensor(X_test_std, DEVICE))\n",
        "    #     test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
        "\n",
        "    #     if len(np.unique(y_test)) == 2:\n",
        "    #         auc = roc_auc_score(y_test, test_probs)\n",
        "    #     else:\n",
        "    #         auc = float(\"nan\")\n",
        "\n",
        "    #     preds = (test_probs >= best['threshold']).astype(np.float32)\n",
        "    #     acc = accuracy_score(y_test, preds)\n",
        "    #     f1 = f1_score(y_test, preds)\n",
        "\n",
        "        # corr = spearmanr(y_test, test_probs).statistic  # regression\n",
        "        # mse = mean_squared_error(y_test, test_probs)\n",
        "        # mae = mean_absolute_error(y_test, test_probs)\n",
        "        # r2 = r2_score(y_test, test_probs)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d} | loss={loss.item():.4f} | train_f1={f1_tr:.3f}\") # | train_acc={acc:.3f} | train_f1={f1:.3f} | train_auc={auc:.3f}\")\n",
        "    # print(f\"Epoch {epoch:02d} | loss={loss.item():.4f} | train_spr={corr_tr:.3f} | train_mse={mse_tr:.3f} | train_mae={mae_tr:.3f} | train_r2={r2_tr:.3f} | test_spr={corr:.3f} | test_mse={mse:.3f} | test_mae={mae:.3f} | test_r2={r2:.3f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Final: use MLP output as confidence score\n",
        "# -----------------------------\n",
        "\n",
        "torch.save(best_mlp, 'quantile_mlp_clf.pt')\n",
        "\n",
        "best_mlp.eval()\n",
        "with torch.inference_mode():\n",
        "    # test_scores = torch.sigmoid(best_mlp(to_tensor(X_test_std, DEVICE))).cpu().numpy()\n",
        "    scores = torch.sigmoid(best_mlp(X_full)).detach().cpu().numpy()\n",
        "\n",
        "# print(f'spearman on test = {spearmanr(test_scores, y_test).statistic}')\n",
        "# print(f'mse on test = {mean_squared_error(y_test, test_scores)}')\n",
        "# print(f'mae on test = {mean_absolute_error(y_test, test_scores)}')\n",
        "# print(f'r2 score on test = {r2_score(y_test, test_scores)}')\n",
        "# print(f'F1 on test = {f1_score(y_test, (test_scores >= best['threshold']).astype(np.float32))}')\n",
        "print(\"\\nExample router scores (higher = more confident 'correct'):\")\n",
        "for i in range(min(20, len(scores))):\n",
        "    print(f\"score={scores[i]:.3f} | y_int={int(y_full[i])} | y={(y_full[i])}\")\n",
        "    # print(f\"score={test_scores[i]:.3f} | y={(y_test[i])} | abs difference={np.abs(test_scores[i]-y_test[i]):.3f}\")  # regression\n",
        "\n",
        "# assert len(scores) == len(results)\n",
        "# for i in range(len(results)):\n",
        "#     results[i]['pred_confidence'] = scores[i]\n",
        "#     results[i]['binary_sim_with_gold'] = int(y_full[i].cpu().numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Train Quantile MLP (regression target)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna, copy\n",
        "\n",
        "# regression cell --> y = y_clf\n",
        "y = copy.deepcopy(y_reg)\n",
        "\n",
        "# -----------------------------\n",
        "# 8) Split + standardize\n",
        "# -----------------------------\n",
        "# X_train, X_test, y_train, y_test = train_test_split(\n",
        "#     X, y.flatten(), test_size=0.25, random_state=SEED, \n",
        "#     stratify=(y if y.sum() > 0 else None)\n",
        "# )\n",
        "\n",
        "# split for tuning...\n",
        "# X_train, X_val, y_train, y_val = train_test_split(\n",
        "#     X, y,\n",
        "#     test_size=0.2,\n",
        "#     # stratify=(y if y.sum() > 0 else None),\n",
        "#     random_state=SEED\n",
        "# )\n",
        "split_idx = int(0.8 * len(X))\n",
        "\n",
        "X_train, X_val = X[:split_idx], X[split_idx:]\n",
        "y_train, y_val = y[:split_idx], y[split_idx:]\n",
        "\n",
        "mu = X_train.mean(axis=0, keepdims=True)\n",
        "sigma = X_train.std(axis=0, keepdims=True) + 1e-6\n",
        "X_train_std = (X_train - mu) / sigma\n",
        "X_val_std = (X_val - mu) / sigma\n",
        "# X_test_std  = (X_test  - mu) / sigma\n",
        "\n",
        "print(f'Train set: {X_train_std.shape, y_train.shape}')\n",
        "print(f'Val set: {X_val_std.shape, y_val.shape}')\n",
        "# print(f'Test set: {X_test_std.shape, y_test.shape}')\n",
        "\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 9) MLP router\n",
        "# -----------------------------\n",
        "class QuantileMLP(nn.Module):\n",
        "    def __init__(self, in_dim=22, h1=64, h2=32, dropout=0.1):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(in_dim, h1),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h1, h2),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(h2, 1),  # logits\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "    \n",
        "\n",
        "class QuantileMLPTunable(nn.Module):\n",
        "    def __init__(self, in_dim, layers, dropout, bn):\n",
        "        super().__init__()\n",
        "\n",
        "        net = []\n",
        "        prev_dim = in_dim\n",
        "        for h in layers:\n",
        "            net.append(nn.Linear(prev_dim, h))\n",
        "            if bn:\n",
        "                net.append(nn.BatchNorm1d(h))\n",
        "            net.append(nn.ReLU())\n",
        "            net.append(nn.Dropout(dropout))\n",
        "            prev_dim = h\n",
        "\n",
        "        net.append(nn.Linear(prev_dim, 1))\n",
        "        self.net = nn.Sequential(*net)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x).squeeze(-1)\n",
        "\n",
        "def pairwise_ranking_loss(scores, targets, eps=1e-4):\n",
        "    \"\"\"\n",
        "    scores: (B,) router outputs (use raw logits, NOT sigmoid)\n",
        "    targets: (B,) continuous quality labels (similarities)\n",
        "    \"\"\"\n",
        "    # Pairwise differences\n",
        "    ds = scores.unsqueeze(1) - scores.unsqueeze(0)   # (B,B)\n",
        "    dq = targets.unsqueeze(1) - targets.unsqueeze(0) # (B,B)\n",
        "\n",
        "    # Keep only informative pairs (avoid ties)\n",
        "    sign = torch.sign(dq)\n",
        "    mask = (dq.abs() > eps)\n",
        "\n",
        "    # Logistic ranking loss: softplus(-sign * ds)\n",
        "    loss_mat = torch.nn.functional.softplus(-sign * ds)\n",
        "    return loss_mat[mask].mean()\n",
        "\n",
        "def log_cosh_loss(y_pred: torch.Tensor, y_true: torch.Tensor) -> torch.Tensor:\n",
        "    def _log_cosh(x: torch.Tensor) -> torch.Tensor:\n",
        "        return x + torch.nn.functional.softplus(-2. * x) - np.log(2.0)\n",
        "    return torch.mean(_log_cosh(y_pred - y_true))\n",
        "\n",
        "class LogCoshLoss(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "\n",
        "    def forward(\n",
        "        self, y_pred: torch.Tensor, y_true: torch.Tensor\n",
        "    ) -> torch.Tensor:\n",
        "        return log_cosh_loss(y_pred, y_true)\n",
        "\n",
        "\n",
        "def objective(trial):\n",
        "    # ----- Architecture search -----\n",
        "    n_layers = trial.suggest_int(\"n_layers\", 1, 5)\n",
        "    layers = [\n",
        "        trial.suggest_int(f\"h{i}\", 16, 512, log=False, step=16)\n",
        "        for i in range(n_layers)\n",
        "    ]\n",
        "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.5)\n",
        "    bn = trial.suggest_categorical(\"batch_norm\", [False, True])\n",
        "\n",
        "    model = QuantileMLPTunable(\n",
        "        in_dim=X_train.shape[1],\n",
        "        layers=layers,\n",
        "        dropout=dropout,\n",
        "        bn=bn\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    # ----- Optimizer -----\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 3e-3, log=True)\n",
        "    wd = trial.suggest_float(\"weight_decay\", 1e-6, 1e-3, log=True)\n",
        "\n",
        "    # threshold = trial.suggest_float(\"threshold\", 0, 1, step=0.1)\n",
        "\n",
        "    opt = torch.optim.AdamW(model.parameters(), lr=lr, weight_decay=wd)\n",
        "    # clf_fn = nn.BCEWithLogitsLoss()  # binary classification\n",
        "    reg_fn = nn.HuberLoss()  # regression\n",
        "\n",
        "    EPOCHS = 250\n",
        "    best_val_score = 0.0\n",
        "\n",
        "    for epoch in range(EPOCHS):\n",
        "        # ---- train ----\n",
        "        model.train()\n",
        "        opt.zero_grad()\n",
        "\n",
        "        logits = model(to_tensor(X_train_std, DEVICE))\n",
        "        # logits = torch.sigmoid(logits)  # if regression!\n",
        "\n",
        "        # loss = clf_fn(logits, to_tensor(y_train, DEVICE))\n",
        "        loss = reg_fn(torch.sigmoid(logits), to_tensor(y_train, DEVICE))\n",
        "        # loss = 0.8 * pairwise_ranking_loss(logits, \n",
        "        #                                    to_tensor(y_train, DEVICE), eps=0.01) + 0.2 * reg_fn(torch.sigmoid(logits), \n",
        "        #                                                                                         to_tensor(y_train, DEVICE))\n",
        "        # loss = pairwise_ranking_loss(logits, to_tensor(y_train, DEVICE))\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "\n",
        "        # ---- validate ----\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            val_logits = model(to_tensor(X_val_std, DEVICE))\n",
        "            val_probs = torch.sigmoid(val_logits).cpu().numpy()\n",
        "\n",
        "            if len(np.unique(y_val)) == 2:\n",
        "                val_score = f1_score(y_val, (val_probs >= threshold).astype(np.float32))\n",
        "            else:\n",
        "                # val_f1 = float(\"nan\")\n",
        "                # val_score = spearmanr(y_val, val_probs).statistic  # regression\n",
        "                # val_score = mean_squared_error(y_val, val_probs)\n",
        "                val_score = mean_absolute_error(y_val, val_probs)\n",
        "                # val_score = r2_score(y_val, val_probs)\n",
        "\n",
        "        best_val_score = max(best_val_score, val_score)\n",
        "\n",
        "        # ---- pruning ----\n",
        "        trial.report(val_score, epoch)\n",
        "        if trial.should_prune():\n",
        "            raise optuna.TrialPruned()\n",
        "\n",
        "    return best_val_score\n",
        "\n",
        "\n",
        "study = optuna.create_study(\n",
        "    direction=\"minimize\",\n",
        "    pruner=optuna.pruners.MedianPruner(n_warmup_steps=10)\n",
        ")\n",
        "\n",
        "study.optimize(objective, n_trials=250)\n",
        "\n",
        "print(\"Best val. score:\", study.best_value)\n",
        "print(\"Best params:\", study.best_params)\n",
        "\n",
        "\n",
        "# define best parameters and train with full data\n",
        "best = study.best_params\n",
        "\n",
        "best_layers = [best[f\"h{i}\"] for i in range(best[\"n_layers\"])]\n",
        "\n",
        "best_mlp = QuantileMLPTunable(\n",
        "    in_dim=X_train_std.shape[1],\n",
        "    layers=best_layers,\n",
        "    dropout=best[\"dropout\"],\n",
        "    bn=best[\"batch_norm\"]\n",
        ").to(DEVICE)\n",
        "\n",
        "opt = torch.optim.AdamW(\n",
        "    best_mlp.parameters(),\n",
        "    lr=best[\"lr\"],\n",
        "    weight_decay=best[\"weight_decay\"]\n",
        ")\n",
        "\n",
        "# clf_fn = nn.BCEWithLogitsLoss()  # binary classification\n",
        "reg_fn = nn.HuberLoss()  # regression\n",
        "\n",
        "X_full = to_tensor(np.vstack((X_train_std, X_val_std)), DEVICE)\n",
        "y_full = to_tensor(np.concatenate((y_train, y_val)), DEVICE)\n",
        "print(f'\\nTrain+Val set: {X_full.shape, y_full.shape}')\n",
        "\n",
        "EPOCHS = 200\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    best_mlp.train()\n",
        "    opt.zero_grad()\n",
        "    \n",
        "    logits = best_mlp(X_full)\n",
        "    # logits = torch.sigmoid(logits)  # regression\n",
        "    \n",
        "    # loss = clf_fn(logits, y_full)\n",
        "    loss = reg_fn(torch.sigmoid(logits), y_full)\n",
        "    # loss = 0.8 * pairwise_ranking_loss(logits, \n",
        "    #                                    to_tensor(y_full, DEVICE), eps=0.01) + 0.2 * reg_fn(torch.sigmoid(logits), \n",
        "    #                                                                                         to_tensor(y_full, DEVICE))\n",
        "    # loss = pairwise_ranking_loss(logits, to_tensor(y_full, DEVICE))\n",
        "    loss.backward()\n",
        "    opt.step()\n",
        "\n",
        "    # f1_tr = f1_score(y_full.cpu().numpy(), (torch.sigmoid(logits).detach().cpu().numpy() >= best['threshold']))\n",
        "    corr_tr = spearmanr(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy()).statistic\n",
        "    mse_tr = mean_squared_error(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "    mae_tr = mean_absolute_error(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "    r2_tr = r2_score(y_full.cpu().numpy(), torch.sigmoid(logits).detach().cpu().numpy())\n",
        "\n",
        "    # best_mlp.eval()\n",
        "    # with torch.no_grad():\n",
        "    #     test_logits = best_mlp(to_tensor(X_test_std, DEVICE))\n",
        "    #     test_probs = torch.sigmoid(test_logits).cpu().numpy()\n",
        "\n",
        "    #     if len(np.unique(y_test)) == 2:\n",
        "    #         auc = roc_auc_score(y_test, test_probs)\n",
        "    #     else:\n",
        "    #         auc = float(\"nan\")\n",
        "\n",
        "    #     preds = (test_probs >= best['threshold']).astype(np.float32)\n",
        "    #     acc = accuracy_score(y_test, preds)\n",
        "    #     f1 = f1_score(y_test, preds)\n",
        "\n",
        "        # corr = spearmanr(y_test, test_probs).statistic  # regression\n",
        "        # mse = mean_squared_error(y_test, test_probs)\n",
        "        # mae = mean_absolute_error(y_test, test_probs)\n",
        "        # r2 = r2_score(y_test, test_probs)\n",
        "\n",
        "    # print(f\"Epoch {epoch:02d} | loss={loss.item():.4f} | train_f1={f1_tr:.3f}\") # | train_acc={acc:.3f} | train_f1={f1:.3f} | train_auc={auc:.3f}\")\n",
        "    print(f\"Epoch {epoch:02d} | loss={loss.item():.4f} | train_spr={corr_tr:.3f} | train_mse={mse_tr:.3f} | train_mae={mae_tr:.3f} | train_r2={r2_tr:.3f} \") # | test_spr={corr:.3f} | test_mse={mse:.3f} | test_mae={mae:.3f} | test_r2={r2:.3f}\")\n",
        "\n",
        "\n",
        "# -----------------------------\n",
        "# 10) Final: use MLP output as confidence score\n",
        "# -----------------------------\n",
        "\n",
        "torch.save(best_mlp, 'quantile_mlp_reg.pt')\n",
        "\n",
        "best_mlp.eval()\n",
        "with torch.inference_mode():\n",
        "    # test_scores = torch.sigmoid(best_mlp(to_tensor(X_test_std, DEVICE))).cpu().numpy()\n",
        "    scores = torch.sigmoid(best_mlp(X_full)).detach().cpu().numpy()\n",
        "\n",
        "# print(f'spearman on test = {spearmanr(test_scores, y_test).statistic}')\n",
        "# print(f'mse on test = {mean_squared_error(y_test, test_scores)}')\n",
        "# print(f'mae on test = {mean_absolute_error(y_test, test_scores)}')\n",
        "# print(f'r2 score on test = {r2_score(y_test, test_scores)}')\n",
        "# print(f'F1 on test = {f1_score(y_test, (test_scores >= best['threshold']).astype(np.float32))}')\n",
        "print(\"\\nExample router scores (higher = more confident 'correct'):\")\n",
        "for i in range(min(20, len(scores))):\n",
        "    print(f\"score={scores[i]:.3f} | y_int={int(y_full[i])} | y={(y_full[i])}\")\n",
        "    # print(f\"score={test_scores[i]:.3f} | y={(y_test[i])} | abs difference={np.abs(test_scores[i]-y_test[i]):.3f}\")  # regression\n",
        "\n",
        "# assert len(scores) == len(results)\n",
        "# for i in range(len(results)):\n",
        "#     results[i]['pred_confidence'] = scores[i]\n",
        "#     results[i]['sim_with_gold'] = y_full[i]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Update results of Train with confidence scores "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_clf_mlp = torch.load('quantile_mlp_clf.pt', weights_only=False).to(DEVICE)\n",
        "best_reg_mlp = torch.load('quantile_mlp_reg.pt', weights_only=False).to(DEVICE)\n",
        "\n",
        "best_clf_mlp.eval()\n",
        "best_reg_mlp.eval()\n",
        "with torch.inference_mode():\n",
        "    # test_scores = torch.sigmoid(best_mlp(to_tensor(X_test_std, DEVICE))).cpu().numpy()\n",
        "    clf_scores = torch.sigmoid(best_clf_mlp(X_full)).detach().cpu().numpy()\n",
        "    reg_scores = torch.sigmoid(best_reg_mlp(X_full)).detach().cpu().numpy()\n",
        "\n",
        "assert len(clf_scores) == len(results) == len(reg_scores)\n",
        "for i in range(len(results)):\n",
        "    results[i]['pred_confidence_clf'] = clf_scores[i]\n",
        "    results[i]['pred_confidence_reg'] = reg_scores[i]\n",
        "    results[i]['sim_with_gold'] = float((y_full[i].cpu().numpy()))\n",
        "    results[i]['binary_with_gold'] = (y_full[i].cpu().numpy() >= SIM_THRESHOLD).astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pickle\n",
        "with open('squad_train_ministral3-3b-it_profiler.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "sns.kdeplot(np.abs(scores-y_full.cpu().numpy()))\n",
        "plt.title(\"Density Plot\")\n",
        "plt.show()\n",
        "\n",
        "sns.kdeplot(sims_all)\n",
        "plt.title(\"Density Plot\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Inference on Validation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "raw = load_dataset(DATASET_ID)\n",
        "print(raw)\n",
        "raw = raw['validation']\n",
        "# raw = raw.select(range(min(MAX_SAMPLES, len(raw))))\n",
        "print(f'Dataset length: {len(raw)}')\n",
        "\n",
        "features_val = []\n",
        "# labels_clf = []\n",
        "labels_reg_val = []\n",
        "# sims_all = []\n",
        "# f1s_all = []\n",
        "\n",
        "results_val = []\n",
        "\n",
        "for start in tqdm(range(0, len(raw), BATCH_SIZE), desc=\"Generating\"):\n",
        "    batch = raw[start:start + BATCH_SIZE]\n",
        "    prompts = [make_prompt2(c, q) for c, q in zip(batch[\"context\"], batch[\"question\"])]\n",
        "\n",
        "    # preds, token_lps_list, results = generate_with_token_logprobs(prompts)  # batch\n",
        "    preds, token_lps_list, r = generate_with_token_logprobs(prompts, return_results=True)\n",
        "\n",
        "    # SQuAD golds are lists of acceptable spans\n",
        "    gold_lists = [ans[\"text\"] if \"text\" in ans else [\"\"] for ans in batch[\"answers\"]]\n",
        "\n",
        "    for i, (pred, golds, lps) in enumerate(zip(preds, gold_lists, token_lps_list)):  # traverse each sample in batch\n",
        "        \n",
        "        r[i]['gold_answer'] = golds[0]\n",
        "\n",
        "        # print('Predicted text:', pred)\n",
        "        # print('Gold text:', golds)\n",
        "        feat = build_posthoc_quantile_features(lps)\n",
        "        sim = max_gold_similarity(pred, golds)\n",
        "        # y_sim_binary = int(sim >= SIM_THRESHOLD)\n",
        "        # y_sim = float(np.clip(sim, 0.0, 1.0))  # regression!\n",
        "        y_sim = float(np.clip((sim + 1.0) / 2.0, 0.0, 1.0))\n",
        "\n",
        "        # f1 = squad_max_f1(pred, golds)\n",
        "        # # f1 = bert_score(pred, golds)\n",
        "        # y_f1 = int(f1 >= SIM_THRESHOLD)\n",
        "        # y_f1 = f1  # regression!\n",
        "\n",
        "        features_val.append(feat)\n",
        "        # # labels.append(y_f1)\n",
        "        # labels_clf.append(y_sim_binary)\n",
        "        labels_reg_val.append(y_sim)\n",
        "        # sims_all.append(sim)\n",
        "        # f1s_all.append(f1)\n",
        "    \n",
        "    results_val.extend(r) if BATCH_SIZE > 1 else results.append(r)\n",
        "        \n",
        "\n",
        "# X = np.stack(features).astype(np.float32)  # (N, 22)\n",
        "\n",
        "# best_mlp.eval()\n",
        "# with torch.inference_mode():\n",
        "#     # test_scores = torch.sigmoid(best_mlp(to_tensor(X_test_std, DEVICE))).cpu().numpy()\n",
        "#     scores = torch.sigmoid(\n",
        "#         best_mlp(to_tensor(np.stack(features).astype(np.float32), DEVICE))\n",
        "#         ).detach().cpu().numpy()\n",
        "\n",
        "# print(\"\\nExample router scores (higher = more confident 'correct'):\")\n",
        "# for i in range(min(20, len(scores))):\n",
        "#     print(f\"score={scores[i]:.3f} | y_int={int(y_full[i])} | y={(y_full[i])}\")\n",
        "#     # print(f\"score={test_scores[i]:.3f} | y={(y_test[i])} | abs difference={np.abs(test_scores[i]-y_test[i]):.3f}\")  # regression\n",
        "\n",
        "# assert len(scores) == len(results)\n",
        "# for i in range(len(results)):\n",
        "#     results[i]['pred_confidence'] = scores[i]\n",
        "#     results[i]['sim_with_gold'] = labels_reg[i]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "best_clf_mlp = torch.load('quantile_mlp_clf.pt', weights_only=False).to(DEVICE)\n",
        "best_reg_mlp = torch.load('quantile_mlp_reg.pt', weights_only=False).to(DEVICE)\n",
        "\n",
        "best_clf_mlp.eval()\n",
        "best_reg_mlp.eval()\n",
        "with torch.inference_mode():\n",
        "    # test_scores = torch.sigmoid(best_mlp(to_tensor(X_test_std, DEVICE))).cpu().numpy()\n",
        "    clf_scores_val = torch.sigmoid(\n",
        "        best_clf_mlp(to_tensor(np.stack(features_val).astype(np.float32), DEVICE))\n",
        "        ).detach().cpu().numpy()\n",
        "    reg_scores_val = torch.sigmoid(\n",
        "        best_reg_mlp(to_tensor(np.stack(features_val).astype(np.float32), DEVICE))\n",
        "        ).detach().cpu().numpy()\n",
        "\n",
        "assert len(clf_scores) == len(results) == len(reg_scores)\n",
        "for i in range(len(results)):\n",
        "    results_val[i]['pred_confidence_clf'] = clf_scores_val[i]\n",
        "    results_val[i]['pred_confidence_reg'] = reg_scores_val[i]\n",
        "    results_val[i]['sim_with_gold'] = labels_reg_val[i]\n",
        "    results_val[i]['binary_with_gold'] = int(labels_reg_val[i] >= SIM_THRESHOLD)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "results_val"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "with open('squad_validation_gemma3-1b-it_profiler.pkl', 'wb') as f:\n",
        "    pickle.dump(results, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# import seaborn as sns\n",
        "# import matplotlib.pyplot as plt\n",
        "\n",
        "# sns.kdeplot(np.abs(scores-y_full.cpu().numpy()))\n",
        "# plt.title(\"Density Plot\")\n",
        "# plt.show()\n",
        "\n",
        "# sns.kdeplot(sims_all)\n",
        "# plt.title(\"Density Plot\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ********************************************************"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "llm_inf_net",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
